{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Assignment 1 Data Mining: \n",
    "    Francesca Cavallini 6971644 \n",
    "    Jozef Siu  \n",
    "    Janneke Hutter 3924734\n",
    "    Koen Hanneman 5743931\n",
    "\n",
    "Growing classification trees, bagging and random forest and predict the class label for given attribute values.\n",
    "Compute quality measures [accuracy, precision, recall] of classifier and confusion matrix.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "from anytree import NodeMixin, Node, RenderTree\n",
    "from anytree.exporter import DotExporter\n",
    "\n",
    "def tree_grow(x, y, nfeat, nmin = 2, minleaf = 1):\n",
    "    \"\"\"\n",
    "    Input parameters:\n",
    "        x (2D array): Data matrix\n",
    "        y (1D array): Binary class labels\n",
    "        nmin (int): Min # observations a node must contain for it to be allowed to split\n",
    "        minleaf (int): Min # observations required for a leaf node\n",
    "        nfeat (int): # features to be considered for each split\n",
    "    Outputs:\n",
    "        Tree object based on best splits of gini index impurity reduction function\n",
    "    \"\"\"\n",
    "    print(\"GROWING CLASSIFICATION TREE\")\n",
    "    #print(f\" x type = {type(x)}, y type = {type(y)}\")\n",
    "    #print(f\"for x = {x[0:20]}, y={y[0:20]}, nmin = {nmin}, minleaf = {minleaf}\")\n",
    "    # each node has a name, list of indices (records), and \"leaf\" boolean attribute\n",
    "    root = Node('root', indices = np.arange(0, x.shape[0]), leaf = False)\n",
    "    nodelist = [root]\n",
    "    split_nr = 0 # will be used for node names\n",
    "    while nodelist: # while nodelist not empty\n",
    "        split_nr += 1\n",
    "        current_node = nodelist.pop(0)  # get node from nodelist TODO: choose random node or first on list?\n",
    "        #print(f\"Processing node  {current_node}\")\n",
    "        # TODO: skip this if nfeat not specified? Adjust optional nfeat in tree_grow def\n",
    "        if nfeat:\n",
    "            feat_list = random.sample(list(np.arange(0, x.shape[1])), k=nfeat)  # randomly draw nfeat col indices from # cols of x\n",
    "        else:\n",
    "            feat_list = list(np.arange(0, x.shape[1]))  # feat_list is simply indices of all columns of x (except first = indices)\n",
    "        \n",
    "        [feat, split_val] = best_split(x,y,current_node, feat_list, minleaf)\n",
    "        \n",
    "        if feat == None and split_value == None : # no possible split found\n",
    "            current_node.leaf = True\n",
    "            # add class prediction label to leaf node:\n",
    "            current_node.y = y[current_node.indices]\n",
    "            if sum((current_node.y) / len(current_node.y)) > 0.5:\n",
    "                current_node.prediction = 1\n",
    "            else:\n",
    "                current_node.prediction = 0\n",
    "        else: # choose split with highest impurity reduction\n",
    "            current_node.split_feat = feat # add feature (col nr of x) and split value by which node will be split\n",
    "            current_node.split_val = split_val\n",
    "            # from indices in current nodes (current_node.indices), select those where value in column f > split_val\n",
    "            indices_left = current_node.indices[x[current_node.indices,feat] > split_val]\n",
    "            left = Node(f\"L{split_nr}\", parent=current_node, indices=indices_left)\n",
    "            # if child node too small for splitting or we have a pure node (impurity=0), make it a leaf node:\n",
    "            if ( len(indices_left) < nmin) or ( impurity(y[indices_left]) == 0):\n",
    "                left.leaf = True\n",
    "                left.y = y[indices_left]\n",
    "                if sum( (left.y) / len(left.y) ) > 0.5:\n",
    "                    left.prediction = 1\n",
    "                else:\n",
    "                    left.prediction = 0\n",
    "            else: # add to nodelist\n",
    "                left.leaf = False\n",
    "                nodelist.append(left)\n",
    "            indices_right = np.setdiff1d(current_node.indices, indices_left)  # indices_right = indices in current node not in indices_left\n",
    "            right = Node(f\"R{split_nr}\", parent=current_node, indices=indices_right)\n",
    "            if ( len(indices_right) < nmin) or ( impurity(y[indices_right]) == 0 ): # make child leaf node\n",
    "                right.leaf = True\n",
    "                right.y = y[indices_right]\n",
    "                if ( sum(right.y) / len(right.y)) > 0.5:\n",
    "                    right.prediction = 1\n",
    "                else:\n",
    "                    right.prediction = 0\n",
    "            else: # add to nodelist\n",
    "                right.leaf = False\n",
    "                nodelist.append(right)\n",
    "    print(f\"TREE DONE\")#\\n {RenderTree(root)}\")\n",
    "    return root\n",
    "\n",
    "def tree_pred(x, tr):\n",
    "    \"\"\"\n",
    "   Input parameters:\n",
    "       x (2D array): Attribute data matrix\n",
    "       tr (AnyTree): Classification tree object\n",
    "   Outputs:\n",
    "       List of predicted labels.\n",
    "   \"\"\"\n",
    "    print(f\"TREE_PRED started\")#for x = \\n {x}\")\n",
    "    n_rows = x.shape[0] # number of rows\n",
    "    y = np.zeros(n_rows)\n",
    "    for i in np.arange(0, n_rows): # for each row = record in x, go down tree\n",
    "        node = tr # start at root node\n",
    "        while not node.leaf: # repeat until we have reached a leaf node\n",
    "            if x[i, node.split_feat] > node.split_val: # go to left child node\n",
    "                node = node.children[0] #go to left child\n",
    "            else:\n",
    "                node = node.children[1] #go to right child\n",
    "        y[i] = node.prediction\n",
    "    return y\n",
    "\n",
    "def tree_grow_b(x, y, m, nfeat, nmin = 2, minleaf = 1):\n",
    "    \"\"\"\n",
    "    Input parameters:\n",
    "        x (2D array): Data matrix\n",
    "        y (1D array): Binary class labels\n",
    "        m (int): number of bootstrapped trees to be made\n",
    "        nmin (int): Min # observations a node must contain for it to be allowed to split\n",
    "        minleaf (int): Min # observations required for a leaf node\n",
    "        nfeat (int): # features to be considered for each split\n",
    "    Outputs:\n",
    "        List of tree objects made from bootstrap samples, each based on best splits of gini index impurity reduction function\n",
    "    \"\"\"\n",
    "    print(f\"STARTING TREE_GROW_B, making {m} bootstrap samples and growing a new tree for each sample\")\n",
    "    trees = [] # list will contain m trees grown from bootstrap samples\n",
    "    i=0\n",
    "    while (i != m):\n",
    "        i+=1\n",
    "        print(f\"Bootstrap {i}\")\n",
    "        n_samples = x.shape[0]\n",
    "        ind = np.random.randint(n_samples, size=n_samples)  # list of indices for bootstrap sample\n",
    "        sample_x = x[ind, :]\n",
    "        sample_y = y[ind]\n",
    "        tree = tree_grow(sample_x, sample_y, nfeat, nmin, minleaf)\n",
    "        trees.append(tree)\n",
    "    return trees\n",
    "\n",
    "def tree_pred_b(x, trees):\n",
    "    \"\"\"\n",
    "    Input parameters:\n",
    "        x (2D array): Attribute data matrix\n",
    "        trees: list of tree objects\n",
    "    Outputs:\n",
    "        list of predicted labels obtained by majority vote of predictions from trees\n",
    "    \"\"\"\n",
    "    n_trees = len(trees) # number of bootstrapped trees\n",
    "    y_predictions = []\n",
    "    for tree in trees:\n",
    "        y_pred = tree_pred(x,tree) # make prediction vector from this tree\n",
    "        y_predictions.append(y_pred) # and add to list of trees\n",
    "    y_predictions = np.array(y_predictions) # convert list to array\n",
    "    # take mean for each column of y_predictions, and see whether mean < 0.5 -> assign prediction = 0\n",
    "    final_pred = list(map(lambda v: 0 if v < 0.5 else 1, np.sum(y_predictions, axis=0)/n_trees))\n",
    "    return final_pred\n",
    "\n",
    "def impurity(x):\n",
    "    \"\"\"\n",
    "    Input parameter:\n",
    "        x: binary vector of class labels\n",
    "    Outputs:\n",
    "        Impurity of that node according to Gini index function\n",
    "    \"\"\"\n",
    "    n = len(x) # records in node\n",
    "    impurity = sum(x)*(n-sum(x))/(n**2)\n",
    "    return impurity\n",
    "\n",
    "def impurity_reduction(parent, left_child, right_child):\n",
    "    \"\"\"\n",
    "    Input parameters:\n",
    "        parent: left_child, right_child: binary class label vectors of parent node and of 2 child nodes of possible split\n",
    "    Outputs:\n",
    "        Impurity reduction value of that split\n",
    "    \"\"\"\n",
    "    impurity_parent = impurity(parent)\n",
    "    impurity_l = impurity(left_child)\n",
    "    impurity_r = impurity(right_child)\n",
    "    imp_red = impurity_parent - ((len(left_child)/len(parent))*impurity_l + (len(right_child)/len(parent))*impurity_r)\n",
    "    return imp_red\n",
    "\n",
    "def bestsplit_of_col(x, y, minleaf):\n",
    "    \"\"\"\n",
    "    Input parameters:\n",
    "        x: numeric attribute vector\n",
    "        y: class label vector\n",
    "        minleaf: minimum size allowed for leaf node\n",
    "    Outputs:\n",
    "        Best split (highest impurity reduction) & split value\n",
    "    \"\"\"\n",
    "    #print(f\"Finding best split of {x} for {y}\")\n",
    "    x_sorted = np.sort(np.unique(x))    #sort x in increasing order, with only unique values\n",
    "    if len(x_sorted) == 1: # All values of vector x are identical\n",
    "        return [0,0]\n",
    "    splitpoints = (x_sorted[0:len(x_sorted)-1]+x_sorted[1:len(x_sorted)])/2\n",
    "    # try all these possible splits:\n",
    "    max_imp_red = 0\n",
    "    best_split_val = splitpoints[0]\n",
    "    for s in list(splitpoints):\n",
    "        indices_left = np.arange(0, len(x)) [x > s] # take row index of elements in x with value > s\n",
    "        indices_right = np.delete(np.arange(0, len(x)), indices_left)  # make list of indices & remove those in indices_left\n",
    "        if (len(indices_left) < minleaf) or (len(indices_right) < minleaf): # child nodes would be too small: no split allowed\n",
    "            continue\n",
    "        left_child = y[indices_left]\n",
    "        right_child = y[indices_right]\n",
    "        imp_red = impurity_reduction(y, left_child, right_child)\n",
    "        if imp_red > max_imp_red:\n",
    "            max_imp_red = imp_red\n",
    "            best_split_val = s\n",
    "    return [max_imp_red, best_split_val]\n",
    "\n",
    "def best_split(x,y,node, feat_list, minleaf):\n",
    "    \"\"\"\n",
    "    Input parameters:\n",
    "        node (Node) : Node that has to be splitted\n",
    "        feat_list (list): Considered features for the best split\n",
    "        minleaf (int): Min # observations required for a leaf node\n",
    "    Outputs:\n",
    "        Feature chosen for the best split (highest impurity reduction) & split value\n",
    "        If there are no possible splits, it returns [None,None] \n",
    "    \"\"\"\n",
    "    poss_splits = []  # will contain for each feature index (in col1), the impurity reduction (col2) & split value (col3) of the best split\n",
    "    for f in feat_list:\n",
    "        # find the best split (based on gini index) for rows of x specific by current_node.indices list, based on col f\n",
    "        #print(f\"Finding best split of column {f}\")\n",
    "        [reduction_val, split_val] = bestsplit_of_col(x[node.indices, f], y[node.indices], minleaf)\n",
    "        if reduction_val != 0:  # if found a split which is allowed, then this is the best split for feature f\n",
    "            poss_splits.append([f, reduction_val, split_val])\n",
    "    if not poss_splits: # no possible split found\n",
    "        return [None,None]\n",
    "    else: # choose split with highest impurity reduction:\n",
    "        poss_splits.sort(key = lambda x: x[1], reverse = True) # sort poss_splits list by the 2nd column (reduction values) in descending order\n",
    "        # TODO: add tiebraker in case of 2 features with identical impurity reduction -> look up previous features via parent?\n",
    "        return [poss_splits[0][0], poss_splits[0][2]]\n",
    "\n",
    "def nodeattrfunc(node):\n",
    "    \"\"\"\n",
    "    Input parameter: node\n",
    "    Outputs: string for labeling that node in the tree picture\n",
    "    \"\"\"\n",
    "    if node.leaf:\n",
    "        return f'label = \"LEAF {node.name}:\\n ind = {node.indices} \\n labels = {node.y} \\n prediction = {node.prediction}\", shape=\"diamond\"'\n",
    "    else:\n",
    "        return f'label = \"Node {node.name}:\\n ind = {node.indices}\"'    # works!\n",
    "\n",
    "def edgeattrfunc(parent, child):\n",
    "    \"\"\"\n",
    "    Input parameters: parent and child node\n",
    "    Outputs: string to label edge between these nodes in the tree picture\n",
    "    \"\"\"\n",
    "    if 'L' in child.name: # we have a left child\n",
    "        return f'label= \"x[:,{parent.split_feat}] > {parent.split_val}\"'\n",
    "    else: # we have a right child\n",
    "        return f'label= \"x[:,{parent.split_feat}] \\u2264 {parent.split_val}\"' # \\u2264 is python source code for <=\n",
    "\n",
    "def compute_metrics(y_true,y_pred):\n",
    "    \"\"\"\n",
    "    Input parameters:\n",
    "        y_true: binary vector of true class labels\n",
    "        y_pred: binary vector of predicted labels\n",
    "    Outputs: [accuracy, precision, recall, cM] with cM = (2x2 array) confusion matrix\n",
    "    \"\"\"\n",
    "    # first compute confusion matrix cM:\n",
    "    T = np.array(y_true,dtype=int) # parse to 1-d array of integers\n",
    "    P = np.array(y_pred,dtype=int)\n",
    "    cM = np.zeros((2,2))\n",
    "    print(f\"MAKING CONFUSION MATRIX \")  # for \\n {y_pred} = predicted, \\n {y_true} = true labels\")\n",
    "    for i in range(len(y_true)):\n",
    "        cM[T[i],P[i]] += 1\n",
    "    TP = cM[0,0]\n",
    "    TN = cM[1,1]\n",
    "    FP = cM[1,0]\n",
    "    FN = cM[0,1]\n",
    "    accuracy = (TP+TN)/np.sum(cM)\n",
    "    precision = TP/(TP+FP)\n",
    "    recall = TP/(TP+FN)\n",
    "    return [accuracy, precision, recall, cM]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Assignment 1 Data Mining: \n",
    "    Francesca Cavallini 6971644 \n",
    "    Jozef Siu  4290216\n",
    "    Janneke Hutter 3924734\n",
    "    Koen Hanneman 5743931\n",
    "\n",
    "Growing classification trees, bagging and random forest and predict the class label for given attribute values.\n",
    "Compute quality measures [accuracy, precision, recall] of classifier and confusion matrix.\n",
    "\n",
    "\"\"\"\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from anytree import NodeMixin, Node, RenderTree\n",
    "from anytree.exporter import DotExporter\n",
    "\n",
    "class TNode(Node):\n",
    "\n",
    "    def set_leaf(self, y):\n",
    "        self.y = y\n",
    "        self.leaf = True\n",
    "        self.prediction = 1 if (sum(y) / len(y)) > 0.5 else 0\n",
    "\n",
    "\n",
    "def tree_grow(x, y, nfeat, nmin = 2, minleaf = 1):\n",
    "    \"\"\" Grows and returns a classification tree\n",
    "\n",
    "    Input Parameters:\n",
    "        x (2D array): Data matrix\n",
    "        y (1D array): Binary class labels\n",
    "        nmin (int): Min # observations a node must contain for it to be allowed to split\n",
    "        minleaf (int): Min # observations required for a leaf node\n",
    "        nfeat (int): # features to be considered for each split\n",
    "\n",
    "    Output Parameters:\n",
    "        Tree object based on best splits of gini index impurity reduction function\n",
    "    \"\"\"\n",
    "    \n",
    "    # each node has a name, list of indices (records), and \"leaf\" boolean attribute\n",
    "    root = Tnode('root', indices = np.arange(0, x.shape[0]), leaf = False) \n",
    "    nodelist = [root]\n",
    "    split_nr = 0 # used for node names\n",
    "    \n",
    "    #nodelist queue\n",
    "    while nodelist:\n",
    "        split_nr += 1\n",
    "        current_node = nodelist.pop(0)  # get node from nodelist TODO: choose random node or first on list?\n",
    "        \n",
    "        feat_list = draw_features(x, nfeat)\n",
    "        [feat, split_val] = best_split(x,y,current_node, feat_list, minleaf) #determine best split\n",
    "        \n",
    "        if feat == None and split_value == None : # no possible split found\n",
    "            current_node.leaf = True\n",
    "            # add class prediction label to leaf node:\n",
    "            current_node.y = y[current_node.indices]\n",
    "            current_node.prediction = class_prediction(current_node.y)\n",
    "\n",
    "        else: # choose split with highest impurity reduction\n",
    "            current_node.split_feat = feat # add feature (col nr of x) and split value by which node will be split\n",
    "            current_node.split_val = split_val\n",
    "            \n",
    "            #set left node\n",
    "            indices_left = current_node.indices[x[current_node.indices,feat] > split_val]\n",
    "            left = TNode(f\"L{split_nr}\", parent=current_node, indices=indices_left)\n",
    "            # if nmin and impurity are not satisfied, make it a leaf node:\n",
    "            if ( len(indices_left) < nmin) or ( impurity(y[indices_left]) == 0):\n",
    "                left.set_leaf(y[indices_left])\n",
    "            else: # add to nodelist\n",
    "                left.leaf = False\n",
    "                nodelist.append(left)\n",
    "\n",
    "            #set right node with \n",
    "            indices_right = np.setdiff1d(current_node.indices, indices_left)  # indices_right = indices in current node not in indices_left\n",
    "            right = TNode(f\"R{split_nr}\", parent=current_node, indices=indices_right)\n",
    "            if ( len(indices_right) < nmin) or ( impurity(y[indices_right]) == 0 ): # make child leaf node\n",
    "                right.set_leaf(y[indices_right])\n",
    "            else: # add to nodelist\n",
    "                right.leaf = False\n",
    "                nodelist.append(right)\n",
    "\n",
    "    return root\n",
    "\n",
    "def draw_features(x, nfeat):\n",
    "    if nfeat:\n",
    "        return random.sample(list(np.arange(0, x.shape[1])), k=nfeat)  # randomly draw nfeat col indices from # cols of x\n",
    "    else:\n",
    "        return list(np.arange(0, x.shape[1]))  # feat_list is simply indices of all columns of x (except first = indices)\n",
    "\n",
    "def class_prediction(y):\n",
    "    classification_ratio = sum(y) / len(y)\n",
    "    return 1 if classification_ratio > 0.5 else 0\n",
    "\n",
    "def tree_pred(x, tr):\n",
    "    \"\"\" Traverses a given tree for all given records to reach a classification prediction\n",
    "\n",
    "   Input parameters:\n",
    "       x (2D array): Attribute data matrix\n",
    "       tr (AnyTree): Classification tree object\n",
    "   Output:\n",
    "       List of predicted labels.\n",
    "   \"\"\"\n",
    "    n_rows = x.shape[0] # number of rows\n",
    "    y_hat = np.zeros(n_rows) #initialize array for predicted labels\n",
    "\n",
    "    for i in np.arange(0, n_rows): # for each row = record in x, go down tree\n",
    "        node = tr # start at root node\n",
    "\n",
    "        while not node.leaf: # repeat until we have reached a leaf node\n",
    "            if x[i, node.split_feat] > node.split_val: # go to left child node\n",
    "                node = node.children[0] #go to left child\n",
    "            else:\n",
    "                node = node.children[1] #go to right child\n",
    "        y_hat[i] = node.prediction\n",
    "        \n",
    "    return y_hat\n",
    "\n",
    "def tree_grow_b(x, y, m, nfeat, nmin = 2, minleaf = 1):\n",
    "    \"\"\" Grows trees for bagging and random forrest\n",
    "\n",
    "    Input parameters:\n",
    "        x (2D array): Data matrix\n",
    "        y (1D array): Binary class labels\n",
    "        m (int): number of bootstrapped trees to be made\n",
    "        nmin (int): Min # observations a node must contain for it to be allowed to split\n",
    "        minleaf (int): Min # observations required for a leaf node\n",
    "        nfeat (int): # features to be considered for each split\n",
    "    Output:\n",
    "        List of tree objects made from bootstrap samples, each based on best splits of gini index impurity reduction function\n",
    "    \"\"\"\n",
    "    trees = [] # list will contain m trees grown from bootstrap samples\n",
    "    i=0\n",
    "    while (i != m):\n",
    "        i+=1\n",
    "        n_samples = x.shape[0]\n",
    "        ind = np.random.randint(n_samples, size=n_samples)  # list of indices for bootstrap sample\n",
    "        sample_x = x[ind, :]\n",
    "        sample_y = y[ind]\n",
    "        tree = tree_grow(sample_x, sample_y, nfeat, nmin, minleaf)\n",
    "        trees.append(tree)\n",
    "    return trees\n",
    "\n",
    "def tree_pred_b(x, trees):\n",
    "    \"\"\" Performs prediction for begging and random forrest\n",
    "\n",
    "    Input parameters:\n",
    "        x (2D array): Attribute data matrix\n",
    "        trees: list of tree objects\n",
    "    Outputs:\n",
    "        list of predicted labels obtained by majority vote of predictions from trees\n",
    "    \"\"\"\n",
    "    n_trees = len(trees) # number of bootstrapped trees\n",
    "    y_predictions = []\n",
    "    for tree in trees:\n",
    "        y_pred = tree_pred(x,tree) # make prediction vector from this tree\n",
    "        y_predictions.append(y_pred) # and add to list of trees\n",
    "    y_predictions = np.array(y_predictions) # convert list to array\n",
    "    # take mean for each column of y_predictions, and see whether mean < 0.5 -> assign prediction = 0\n",
    "    final_pred = list(map(lambda v: 0 if v < 0.5 else 1, np.sum(y_predictions, axis=0)/n_trees))\n",
    "    return final_pred\n",
    "\n",
    "def impurity(x):\n",
    "    \"\"\"\n",
    "    Input parameter:\n",
    "        x: binary vector of class labels\n",
    "    Outputs:\n",
    "        Impurity of that node according to Gini index function\n",
    "    \"\"\"\n",
    "    n = len(x) # records in node\n",
    "    impurity = sum(x)*(n-sum(x))/(n**2)\n",
    "    return impurity\n",
    "\n",
    "def impurity_reduction(parent, left_child, right_child):\n",
    "    \"\"\"\n",
    "    Input parameters:\n",
    "        parent: left_child, right_child: binary class label vectors of parent node and of 2 child nodes of possible split\n",
    "    Outputs:\n",
    "        Impurity reduction value of that split\n",
    "    \"\"\"\n",
    "    impurity_parent = impurity(parent)\n",
    "    impurity_l = impurity(left_child)\n",
    "    impurity_r = impurity(right_child)\n",
    "    imp_red = impurity_parent - ((len(left_child)/len(parent))*impurity_l + (len(right_child)/len(parent))*impurity_r)\n",
    "    return imp_red\n",
    "\n",
    "\n",
    "def best_split(x,y,node, feat_list, minleaf):\n",
    "    \"\"\"\n",
    "    Input parameters:\n",
    "        node (Node) : Node that has to be splitted\n",
    "        feat_list (list): Considered features for the best split\n",
    "        minleaf (int): Min # observations required for a leaf node\n",
    "    Outputs:\n",
    "        Feature chosen for the best split (highest impurity reduction) & split value\n",
    "        If there are no possible splits, it returns [None,None] \n",
    "    \"\"\"\n",
    "    poss_splits = []  # will contain for each feature index (in col1), the impurity reduction (col2) & split value (col3) of the best split\n",
    "    for f in feat_list:\n",
    "        # find the best split (based on gini index) for rows of x specific by current_node.indices list, based on col f\n",
    "        [reduction_val, split_val] = bestsplit_of_col(x[node.indices, f], y[node.indices], minleaf)\n",
    "        if reduction_val != 0:  # if found a split which is allowed, then this is the best split for feature f\n",
    "            poss_splits.append([f, reduction_val, split_val])\n",
    "    \n",
    "    if not poss_splits: # no possible split found\n",
    "        return [None,None]\n",
    "    else: \n",
    "        poss_splits.sort(key = lambda x: x[1], reverse = True) # sort poss_splits list by the 2nd column (reduction values) in descending order\n",
    "        return [poss_splits[0][0], poss_splits[0][2]] # Return split with highest impurity reduction:\n",
    "\n",
    "def bestsplit_of_col(x, y, minleaf):\n",
    "    \"\"\"\n",
    "    Input parameters:\n",
    "        x: numeric attribute vector\n",
    "        y: class label vector\n",
    "        minleaf: minimum size allowed for leaf node\n",
    "    Outputs:\n",
    "        Best split (highest impurity reduction) & split value\n",
    "    \"\"\"\n",
    "    x_sorted = np.sort(np.unique(x))    #sort x in increasing order, with only unique values\n",
    "\n",
    "    if len(x_sorted) == 1: # All values of vector x are identical\n",
    "        return [0,0]\n",
    "\n",
    "    splitpoints = (x_sorted[0:len(x_sorted)-1]+x_sorted[1:len(x_sorted)])/2\n",
    "    # try all these possible splits:\n",
    "    max_imp_red = 0\n",
    "    best_split_val = splitpoints[0]\n",
    "    for s in list(splitpoints):\n",
    "        indices_left = np.arange(0, len(x)) [x > s] # take row index of elements in x with value > s\n",
    "        indices_right = np.delete(np.arange(0, len(x)), indices_left)  # make list of indices & remove those in indices_left\n",
    "        \n",
    "        if (len(indices_left) < minleaf) or (len(indices_right) < minleaf): # child nodes would be too small: no split allowed\n",
    "            continue\n",
    "        left_child = y[indices_left]\n",
    "        right_child = y[indices_right]\n",
    "\n",
    "        imp_red = impurity_reduction(y, left_child, right_child)\n",
    "        if imp_red > max_imp_red:\n",
    "            max_imp_red = imp_red\n",
    "            best_split_val = s\n",
    "    return [max_imp_red, best_split_val]\n",
    "\n",
    "\n",
    "def nodeattrfunc(node):\n",
    "    \"\"\"\n",
    "    Input parameter: node\n",
    "    Outputs: string for labeling that node in the tree picture\n",
    "    \"\"\"\n",
    "    if node.leaf:\n",
    "        return f'label = \"LEAF {node.name}:\\n ind = {node.indices} \\n labels = {node.y} \\n prediction = {node.prediction}\", shape=\"diamond\"'\n",
    "    else:\n",
    "        return f'label = \"Node {node.name}:\\n ind = {node.indices}\"'    # works!\n",
    "\n",
    "def edgeattrfunc(parent, child):\n",
    "    \"\"\"\n",
    "    Input parameters: parent and child node\n",
    "    Outputs: string to label edge between these nodes in the tree picture\n",
    "    \"\"\"\n",
    "    if 'L' in child.name: # we have a left child\n",
    "        return f'label= \"x[:,{parent.split_feat}] > {parent.split_val}\"'\n",
    "    else: # we have a right child\n",
    "        return f'label= \"x[:,{parent.split_feat}] \\u2264 {parent.split_val}\"' # \\u2264 is python source code for <=\n",
    "\n",
    "def compute_metrics(y_true,y_pred):\n",
    "    \"\"\"\n",
    "    Input parameters:\n",
    "        y_true: binary vector of true class labels\n",
    "        y_pred: binary vector of predicted labels\n",
    "    Outputs: [accuracy, precision, recall, cM] with cM = (2x2 array) confusion matrix\n",
    "    \"\"\"\n",
    "    # compute confusion matrix cM:\n",
    "    T = np.array(y_true,dtype=int) # parse to 1-d array of integers\n",
    "    P = np.array(y_pred,dtype=int)\n",
    "    cM = np.zeros((2,2))\n",
    "\n",
    "    for i in range(len(y_true)):\n",
    "        cM[T[i],P[i]] += 1\n",
    "\n",
    "    #compute metrics\n",
    "    TP = cM[0,0]\n",
    "    TN = cM[1,1]\n",
    "    FP = cM[1,0]\n",
    "    FN = cM[0,1]\n",
    "    accuracy = (TP+TN)/np.sum(cM)\n",
    "    precision = TP/(TP+FP)\n",
    "    recall = TP/(TP+FN)\n",
    "    return [accuracy, precision, recall, cM]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def process_csv(path):\n",
    "\n",
    "    data = pd.read_csv(path, sep=';')\n",
    "    column_names = data.columns\n",
    "    predictor_names = ['FOUT', 'MLOC', 'NBD', 'PAR', 'VG', 'NOF', 'NOM', 'NSF', 'NSM', 'ACD', 'NOI', 'NOT', 'TLOC',\n",
    "                       'NOCU']\n",
    "    # select all columns name containing any of the strings in the above list plus column 'pre' (pre-bugs) :\n",
    "    select_predictors = [col for col in column_names if col == 'pre' or any(x in col for x in predictor_names)]\n",
    "\n",
    "    X = data[select_predictors]\n",
    "    post_bugs = data['post']  # number of post-release bugs\n",
    "    y_train = [y if y == 0 else 1 for y in post_bugs]\n",
    "    y_train = np.array(y_train)\n",
    "\n",
    "    X = X.to_numpy()  # parse pandas DataFrame to numpy array\n",
    "    return [X, y_train]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def single_tree():\n",
    "    tree = tree_grow(X_train, y_train, nfeat=X_train.shape[1], nmin = 15, minleaf=5)\n",
    "    y_pred = tree_pred(X_test, tree)\n",
    "    print_metrics(\"Single_tree\", y_test, y_pred)\n",
    "\n",
    "def bagging():\n",
    "    trees = tree_grow_b(X_train, y_train, 100, nfeat=X_train.shape[1], nmin = 15, minleaf=5)\n",
    "    y_pred = tree_pred_b(X_test, trees)\n",
    "    print_metrics(\"Bagging\", y_test, y_pred)\n",
    "\n",
    "def random_forrest():\n",
    "    trees = tree_grow_b(X_train, y_train, 100, nfeat=6, nmin=15, minleaf=5)\n",
    "    y_pred = tree_pred_b(X_test, trees)\n",
    "    print_metrics(\"Random_forrest\", y_test, y_pred)\n",
    "\n",
    "def print_metrics(model, y_test, y_pred):\n",
    "    [accuracy, precision, recall, cM] = compute_metrics(y_test, y_pred)\n",
    "    print(f\"Metrics for the {model} are:\\n accuracy = {accuracy}, precision = {precision}, recall = {recall}, confusion matrix = \\n {cM}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Metrics for the Single_tree are:\n accuracy = 0.6822995461422088, precision = 0.6751269035532995, recall = 0.764367816091954, confusion matrix = \n [[266.  82.]\n [128. 185.]]\nMetrics for the Bagging are:\n accuracy = 0.7655068078668684, precision = 0.7347931873479319, recall = 0.867816091954023, confusion matrix = \n [[302.  46.]\n [109. 204.]]\nMetrics for the Random_forrest are:\n accuracy = 0.75642965204236, precision = 0.7441253263707572, recall = 0.8189655172413793, confusion matrix = \n [[285.  63.]\n [ 98. 215.]]\n"
    }
   ],
   "source": [
    "######################\n",
    "##Execute Algorithms##\n",
    "######################\n",
    "\n",
    "# [X_train, y_train] = process_csv(\"promise-2_0a-packages-csv/eclipse-metrics-packages-2.0.csv\")\n",
    "# [X_test, y_test] = process_csv(\"promise-2_0a-packages-csv/eclipse-metrics-packages-3.0.csv\")\n",
    "\n",
    "# single_tree()\n",
    "# bagging()\n",
    "# random_forrest()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "Python 3.8.5 64-bit",
   "display_name": "Python 3.8.5 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "1ee38ef4a5a9feb55287fd749643f13d043cb0a7addaab2a9c224cbe137c0062"
    }
   }
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}